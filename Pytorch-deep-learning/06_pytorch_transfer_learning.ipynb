{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNzqlFkzALEEaPeQsJgIr2V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bolorooo24/CV-ML_labs/blob/main/Pytorch-deep-learning/06_pytorch_transfer_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 06. Pytorch Transfer Learning\n",
        "\n",
        "What is transfer learning?\n",
        "\n",
        "Transfer learning involves taking the parameters of what one model has learned on another dataset and applying to our own problem.\n",
        "\n",
        "* Pretrained model = foundation models.\n"
      ],
      "metadata": {
        "id": "ClWtGBuxlxXQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u9PjFxT5lsxG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "\n",
        "print(torch.__version__)\n",
        "print(torchvision.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we've go the version of torch and torchvision, let;s import the code we've written in previous sections so that we don't have to write it all again."
      ],
      "metadata": {
        "id": "KL7QbqYjnOXM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# regular import\n",
        "import torch\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "\n",
        "# try to get torchinfo, install it if it doesn't work\n",
        "\n",
        "try:\n",
        "  from torchinfo import summary\n",
        "except:\n",
        "  print(\"info : coundn't find torvhinfo.. installing it\")\n",
        "  !pip install -q torchinfo\n",
        "  from torchinfo import summary\n",
        "\n",
        "# try to import the going_modular directory, download it from github if it doesn't work\n",
        "try:\n",
        "  from going_modular.going_modular import data_setup, engine\n",
        "\n",
        "except:\n",
        "  # get the going_modular scripts\n",
        "  print(\"info: cound't find going_modular scripts .. downloading them from github\")\n",
        "  !git clone https://github.com/mrdbourke/pytorch-deep-learning\n",
        "  !mv pytorch-deep-learning/going_modular .\n",
        "  !rm -rf pytorch-deep-learning\n",
        "  from going_modular.going_modular import data_setup, engine"
      ],
      "metadata": {
        "id": "_-tdvQZvoUPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# setup device agnostic code\n",
        "device = \"cuda\" if torch.cuda.is_available else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "id": "k22qeBZcpZ13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "hrSfQcFcqICD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Get Data\n",
        "\n",
        "We need our pizza steak sushi data."
      ],
      "metadata": {
        "id": "klYCbFaKp22f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "import requests\n",
        "\n",
        "\n",
        "# setup data path\n",
        "data_path = Path(\"data/\")\n",
        "image_path = data_path / \"pizza_steak_sushi\" # images from a subset of classes from the dataset\n",
        "\n",
        "# if the image folder doesn't exitst, download it\n",
        "\n",
        "if image_path.is_dir():\n",
        "  print(\"The image directory already exists, skipping download.\")\n",
        "else:\n",
        "  print(f\"Did not find {image_path}\")\n",
        "  image_path.mkdir(parents=True, exist_ok = True)\n",
        "\n",
        "\n",
        "  with open(data_path / \"pizza_steak_sushi.zip\", \"wb\") as f:\n",
        "\n",
        "    request = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/refs/heads/main/data/pizza_steak_sushi.zip\")\n",
        "    print(\"Downloading pizza, steak, sushi data ...\")\n",
        "    f.write(request.content)\n",
        "\n",
        "  with zipfile.ZipFile(data_path/\"pizza_steak_sushi.zip\", \"r\") as zipfile:\n",
        "    print(\"Unzipping the data \")\n",
        "    zipfile.extractall(image_path)\n",
        "\n",
        "  # remove .zip file\n",
        "  os.remove(data_path/\"pizza_steak_sushi.zip\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "muYwr4Q6qJt5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# setup directory path\n",
        "train_dir = image_path/\"train\"\n",
        "test_dir = image_path/\"test\"\n",
        "\n",
        "train_dir, test_dir"
      ],
      "metadata": {
        "id": "HdYQVB-vHKjX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Create datasets and dataloaders\n",
        "\n",
        "Next step is turn data into Pytorch dataloaders\n",
        "\n",
        "To do so, we can use `data_setup.py` and the `create_dataloaders()` function we made in chapter 5.\n",
        "\n",
        "\n",
        "There's one hting we have to think about when lading: how ot **transform** it?\n",
        "\n",
        "And with `torchvision` 0.13+ there's two ways to do this:\n",
        "\n",
        "1. Manually created transforms - you define what transforms you want your data to go through\n",
        "2. Automatically created transforms - the transforms for your data are defined by the model you'd like to use.\n",
        "\n",
        "Important point: when using a pretrained model, it's important that the data (including your custom data) that you pass through it is **transormed** in the same way that the data the model was trained on.\n"
      ],
      "metadata": {
        "id": "twu5qY441cfp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from going_modular.going_modular import data_setup\n",
        "\n"
      ],
      "metadata": {
        "id": "SrPy4iDj2JVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Creating a transform for `torchvision.models` (manual creation)\n",
        "\n",
        "`torchvision.models` contains pretrained models (models ready for transfer learning) right within `torchvision`\n",
        "\n",
        "> All pretrained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3xHxW), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0,1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.224]. You can use the following transform to normalize."
      ],
      "metadata": {
        "id": "72gLSjKB4EvH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std = [0.229, 0.224, 0.225])\n",
        "\n",
        "manual_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)), # resize image to 224, 224\n",
        "    transforms.ToTensor(), # get images into range [0,1]\n",
        "    normalize # make sure images have the same distribution as ImageNet (where our pretrianed models have been trained)\n",
        "])"
      ],
      "metadata": {
        "id": "M19nAcps4vSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir = train_dir,\n",
        "                                                                               test_dir = test_dir,\n",
        "                                                                               transform = manual_transforms,\n",
        "                                                                                batch_size = 32)\n",
        "\n",
        "train_dataloader, test_dataloader, class_names\n",
        "\n"
      ],
      "metadata": {
        "id": "vPYJtkt98pMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Creating a tranform for `torchvision.models` (auto creation)\n",
        "\n",
        "As of +13 torchvision, there is now support for automatic data transform creation based on the pretrained model weights you're using"
      ],
      "metadata": {
        "id": "PuRUSrIH89kn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "torchvision.__version__"
      ],
      "metadata": {
        "id": "0Li4vuni9pSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get aset of pretrained model weights\n",
        "\n",
        "weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT # Default = best performing weights\n",
        "weights"
      ],
      "metadata": {
        "id": "aIgjBBSH9tDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get the transforms used to create our pretrained weights\n",
        "auto_transforms = weights.transforms()\n",
        "auto_transforms"
      ],
      "metadata": {
        "id": "qQIZZzUMhF0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create dataloaders using aumatic transforms\n",
        "\n",
        "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n",
        "    train_dir = train_dir,\n",
        "    test_dir = test_dir,\n",
        "    transform = auto_transforms,\n",
        "    batch_size = 32\n",
        ")\n",
        "train_dataloader, test_dataloader, class_names\n"
      ],
      "metadata": {
        "id": "WTpH7SxfhP2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Getting a pretrained model\n",
        "\n",
        "There are various places to get a pretrained model such as:\n",
        "1. PyTorch domain libraries\n",
        "2. Lib like `timm` (torch image models)\n",
        "3. HuggingFace Hub (for models across different problem scpaces/domains)\n",
        "4. Paperswithcode (for models across different problem spaces/domains)"
      ],
      "metadata": {
        "id": "YPSRUr3mhxUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Which pretrained model should you use?\n",
        "\n",
        "*Experiment, experiment, experiment!*\n",
        "\n",
        "The whole idea of transfer learning : take an already well-performing model from a problem space silimar to your own and then customize to your own problem.\n",
        "\n",
        "3 things to consider:\n",
        "1. Speed -how fast does it run?\n",
        "2. Size - how big is the model?\n",
        "3. Performance - how well does it go on your problem?\n",
        "\n",
        "Where does the model live?\n",
        "\n",
        "Is it on device?\n",
        "Or does it live on a server?\n",
        "\n",
        "Looking at https://docs.pytorch.org/vision/main/models.html\n",
        "\n",
        "Which model should we choose?\n",
        "\n",
        "For our case(deploying FoodVision Mini on a mobile device), it looks like EffNetB0 is one of our best options in terms performance vs size.\n",
        "\n",
        "However, in light of the Bitter Lesson, if we had infinite compute, we'd likely pick the biggest model + most parameters + most general we could."
      ],
      "metadata": {
        "id": "fbm3P8syij-m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Setting up a pretrained model\n",
        "\n",
        "Want to creaet an instance of a pretrained EfficientNetB0\n",
        "\n",
        "https://docs.pytorch.org/vision/main/models/generated/torchvision.models.efficientnet_b0.html#torchvision.models.efficientnet_b0"
      ],
      "metadata": {
        "id": "T4ljxrQxjdDc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# old method of creating a pretraine model prior to torchvision v0.13\n",
        "\n",
        "model = torchvision.models.efficientnet_b0(pretrained=True)\n",
        "\n",
        "# new method of creating a pretrained model\n",
        "weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n",
        "model = torchvision.models.efficientnet_b0(weights=weights).to(device)\n",
        "model"
      ],
      "metadata": {
        "id": "JN3cn-rcahuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.features"
      ],
      "metadata": {
        "id": "aMruwtf7bIpb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.classifier"
      ],
      "metadata": {
        "id": "WNGThFPpb2Pq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 Getting a summary of our model with `torchinfo.summary()`"
      ],
      "metadata": {
        "id": "A-LXHnueb5Jh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary\n",
        "\n",
        "summary(model=model,\n",
        "        input_size=(1, 3, 224, 224), # example of [batch_size, color_channels, height, width])\n",
        "        col_names = [\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "        col_width = 20,\n",
        "        row_settings=[\"var_names\"])"
      ],
      "metadata": {
        "id": "7xtLBybEeHfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4 Freezing the base model and changing the output layer to suit our need\n",
        "\n",
        "With a feature extractor model, typically you will \"freeze\" the base layers of a pretrained/foundation model and update the outptu layers to suit your own problem."
      ],
      "metadata": {
        "id": "o399rI_xetmF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# freeze all of the base layers in EffNetB0\n",
        "for param in model.features.parameters():\n",
        "  param.requires_grad = False"
      ],
      "metadata": {
        "id": "qHC0nKbtfpxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# update the classifier head of our model to suit our problem\n",
        "from torch import nn\n",
        "\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "model.classifier = nn.Sequential(\n",
        "    nn.Dropout(p=0.2, inplace=True),\n",
        "    nn.Linear(in_features=1280, # feature vector coming in\n",
        "              out_features=len(class_names))).to(device)\n",
        "\n",
        "model.classifier"
      ],
      "metadata": {
        "id": "hPZRm3nVgQ_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gAg8zRbNgghW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary(model=model,\n",
        "        input_size=(1, 3, 224, 224), # example of [batch_size, color_channels, height, width])\n",
        "        col_names = [\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "        col_width = 20,\n",
        "        row_settings=[\"var_names\"])"
      ],
      "metadata": {
        "id": "12ffngnBf5YO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.0 Train model"
      ],
      "metadata": {
        "id": "Jf5AUpM5gLoP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define loss and optimizer\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(params=model.parameters(),\n",
        "                             lr=0.001)\n"
      ],
      "metadata": {
        "id": "UMSw4kE44zkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import train function\n",
        "from going_modular.going_modular import engine\n",
        "\n",
        "# set the manual seeds\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "# start timer\n",
        "from timeit import default_timer as timer\n",
        "start_time = timer()\n",
        "\n",
        "# setup training and save the results\n",
        "results = engine.train(model=model,\n",
        "                       train_dataloader=train_dataloader,\n",
        "                       test_dataloader=test_dataloader,\n",
        "                       optimizer=optimizer,\n",
        "                       loss_fn=loss_fn,\n",
        "                       epochs=5,\n",
        "                       device=device)\n",
        "# end the timer and print out how long it took\n",
        "end_time = timer()\n",
        "print(f\"[INFO] Total training time: {end_time-start_time:.3f} seconds\")\n"
      ],
      "metadata": {
        "id": "5R--rFyd5DD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Evaluate model by plotting loss curve\n"
      ],
      "metadata": {
        "id": "eDsSK8r66ISv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  from helper_functions import plot_loss_curves\n",
        "except:\n",
        "  print(f\"couldn't find helper_function.py, downloading ...\")\n",
        "  with open(\"helper_functions.py\", \"wb\") as f:\n",
        "    import requests\n",
        "    request = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/refs/heads/main/helper_functions.py\")\n",
        "    f.write(request.content)\n",
        "  from helper_functions import plot_loss_curves\n",
        "\n",
        "  # plot the loss curves of our model\n",
        "plot_loss_curves(results)"
      ],
      "metadata": {
        "id": "Ss0irugd6q3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Make predictions on images from the test set\n",
        "\n",
        "Let's adhere to the data explorer's motto of \"visualize,,,,\"\n",
        "\n",
        "And make some qualitiative predictions on our test set.\n",
        "\n",
        "Some things to keep in mind when making predictions/inference on test data/custom data.\n",
        "\n",
        "We have to make sure that our test/custom data is:\n",
        "* Same shape - images need to be same shape as model was trained on\n",
        "* Same datatype - custom data should be in the same data type\n",
        "* Same device - custom data/test data shoudl be on the same device as the model\n",
        "* Same transform - if you've transformed your custom data, ideally you will transform the test data and custom data the same\n",
        "\n",
        "\n",
        "To do all of this automagically, let's create a function called `pred_and_plot_image()`\n",
        "\n",
        "The function will be similar to the one in the previous code.\n",
        "\n",
        "1. Take in a trained model, a list of calss names, a filepath to a target image, an image size, a transform anda target device\n",
        "2. Open the image with `PIL.Image.Open()`\n",
        "3. Create a transform if one doesn't exit\n",
        "4. Make sure the model is on the target device\n",
        "5. Turn the model to `model.eval()` mode to make sure it's ready for inference (this will turn off things like `nn.Dropout()`)\n",
        "6. Transform the target image and make sure its dimension is suited for the model (this mainly relates to batch size)\n",
        "7. Make a prediction on the image by passing to the model\n",
        "8. Conver the model's output logits to prediction probs usin `torch.softmax()`\n",
        "9. Convert model's prediction probs to prediction labels using `torch.argmax()`\n",
        "10. Plot the image with `matplotlib` and set the title to the prediction label from step 9 and prediction probs from step 8\n",
        "\n"
      ],
      "metadata": {
        "id": "g-G9n2wH7nDJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Tuple\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "# 1. Take in a trained model...\n",
        "def pred_and_plot_image(\n",
        "    model: torch.nn.Module,\n",
        "    image_path: str,\n",
        "    class_names: List[str],\n",
        "    image_size: Tuple[int, int] = (244, 244),\n",
        "    transform: torchvision.transforms=None,\n",
        "    device: torch.device = device):\n",
        "  # 2. open the image with PIL\n",
        "  img = Image.open(image_path)\n",
        "\n",
        "  # 3. create transform\n",
        "  if transform is not None:\n",
        "    image_transform = transform\n",
        "  else:\n",
        "    image_transform = transforms.Compose([\n",
        "        transforms.Resize(image_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                             std = [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "  # 4. make sure the model on the target device\n",
        "  model.to(device)\n",
        "\n",
        "  # 5. turn on inference mode and eval mode\n",
        "  model.eval()\n",
        "  with torch.inference_mode():\n",
        "    # 6. transform the image and add an extra batch dimension\n",
        "    transformed_image = image_transform(img).unsqueeze(dim=0) # [batch_size, color_channels, height, width]\n",
        "\n",
        "    # 7. make a prediction on the transformed image by passing it to the model and ensure the target device\n",
        "    target_image_pred = model(transformed_image.to(device))\n",
        "\n",
        "  # 8. convert the model's output logits to pred probs\n",
        "  target_image_pred_probs = torch.softmax(target_image_pred, dim=1)\n",
        "\n",
        "  # 9. convert the model's pred probs to pred labels\n",
        "  target_image_pred_labels = torch.argmax(target_image_pred_probs, dim =1)\n",
        "\n",
        "  # 10. plot image with predicted label and prob\n",
        "  plt.figure()\n",
        "  plt.imshow(img)\n",
        "  plt.title(f\"Pred: {class_names[target_image_pred_labels]} | Prob: {target_image_pred_probs.max():.3f}\")\n",
        "  plt.axis(False);\n"
      ],
      "metadata": {
        "id": "5GCCwZCA9Dq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "laNnVY7x14Hc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import os\n",
        "\n",
        "# get a random list of image paths the test set\n",
        "def get_random_images(dataset_dir: str,\n",
        "                      number_of_images: int):\n",
        "  image_files = []\n",
        "  if dataset_dir:\n",
        "    for root, dirs, files in os.walk(dataset_dir):\n",
        "      for file in files:\n",
        "        img_path = os.path.join(root,file)\n",
        "        image_files.append(img_path)\n",
        "    random_images = random.choices(image_files, k=number_of_images)\n",
        "    return random_images\n",
        "\n"
      ],
      "metadata": {
        "id": "fqGtyQKCx0n4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_images_to_plot = 3\n",
        "test_image_path_list = list(Path(test_dir).glob(\"*/*.jpg\"))\n",
        "test_image_path_sample  = random.sample(population=test_image_path_list,\n",
        "                                        k=num_images_to_plot)\n",
        "test_image_path_sample"
      ],
      "metadata": {
        "id": "pogyp21k2DkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_images = get_random_images(test_dir, 3)\n",
        "\n",
        "for image_path in random_images:\n",
        "  pred_and_plot_image(\n",
        "      model=model,\n",
        "      image_path = image_path,\n",
        "      class_names = class_names,\n",
        "      image_size=(244,244)\n",
        "  )"
      ],
      "metadata": {
        "id": "4AhLIaP107dt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.1 Making prediction on a custom image\n",
        "\n",
        "let's make prediction on the pizza dad image: https://github.com/mrdbourke/pytorch-deep-learning/blob/baa27b691617461c173ecf3439793cd8cde74e13/images/04-pizza-dad.jpeg\n"
      ],
      "metadata": {
        "id": "1m-vGHZVyAB-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "custom_image_path = data_path/ \"04-pizza-dad.jpeg\"\n",
        "\n",
        "if not custom_image_path.is_file():\n",
        "  with open(custom_image_path, \"wb\") as f:\n",
        "    request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pizza-dad.jpeg\")\n",
        "    print(f\"Downloading ...\")\n",
        "    f.write(request.content)\n",
        "else:\n",
        "  print(f\"{custom_image_path} already exists, skipping download ...\")\n",
        ""
      ],
      "metadata": {
        "id": "2-Zn9jJZ3zsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_and_plot_image(model=model,\n",
        "                    image_path=custom_image_path,\n",
        "                    class_names=class_names)"
      ],
      "metadata": {
        "id": "31yn9kC9NNTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zfN1q2FkNquH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}