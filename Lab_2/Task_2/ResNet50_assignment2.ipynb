{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bolorooo24/CV-ML_labs/blob/main/Lab_2/Task_2/ResNet50_assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "k7t1PKrFT1Ai"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kS7gGSqTgRgy",
        "outputId": "58685acb-e923-45e2-b215-dcb5d2cd7325"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "z1sAIPIBUct0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])"
      ],
      "metadata": {
        "id": "jUrvhzrVUfAc"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64"
      ],
      "metadata": {
        "id": "0n1yusnKUh22"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.datasets as datasets\n",
        "dataset = datasets.CIFAR100(root='./cifar100', train=True, download=True, transform=transform)"
      ],
      "metadata": {
        "id": "02BnNQAhViqD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "164a1718-f64d-4dcd-9398-63dc3ffe945a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./cifar100/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169M/169M [00:05<00:00, 31.0MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./cifar100/cifar-100-python.tar.gz to ./cifar100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# put dataset structure into dictionary\n",
        "classes_dict  = {}\n",
        "\n",
        "for img, label in dataset:\n",
        "  if label not in classes_dict:\n",
        "    classes_dict[label] = []\n",
        "  classes_dict[label].append(img)"
      ],
      "metadata": {
        "id": "cFLCa64DWVip"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create subset of the dataset\n",
        "import random\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def subset_create(classes_dict, num_classes, num_samples):\n",
        "    sample_classes = random.sample(list(classes_dict.keys()), num_classes)\n",
        "    train_data, test_data, train_labels, test_labels = [], [], [], []\n",
        "    class_mapping = {cls: idx for idx, cls in enumerate(sample_classes)}\n",
        "\n",
        "    for c in sample_classes:\n",
        "        sample_images = random.sample(classes_dict[c], num_samples)\n",
        "\n",
        "        train_imgs, test_imgs = train_test_split(sample_images, test_size=0.2, random_state=42)\n",
        "        train_data.extend(train_imgs)\n",
        "        train_labels.extend([class_mapping[c]] * len(train_imgs))\n",
        "        test_data.extend(test_imgs)\n",
        "        test_labels.extend([class_mapping[c]] * len(test_imgs))\n",
        "\n",
        "    train_dataset = TensorDataset(torch.stack(train_data), torch.tensor(train_labels))\n",
        "    test_dataset = TensorDataset(torch.stack(test_data), torch.tensor(test_labels))\n",
        "    return train_dataset, test_dataset"
      ],
      "metadata": {
        "id": "reZwtJM8W3Ay"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create each subset\n",
        "\n",
        "subsets = {\n",
        "    \"10x10\": subset_create(classes_dict, 10, 10),\n",
        "    \"10x100\": subset_create(classes_dict, 10, 100),\n",
        "    \"10x200\": subset_create(classes_dict, 10, 200),\n",
        "    \"100x10\": subset_create(classes_dict, 100, 10),\n",
        "    \"100x100\": subset_create(classes_dict, 100, 100),\n",
        "    \"100x200\": subset_create(classes_dict, 100, 200),\n",
        "}\n",
        "print(\"subset created!!\")\n",
        "\n",
        "for key in subsets:\n",
        "    train_subset, test_subset = subsets[key]\n",
        "    subsets[key] = (train_subset, test_subset)"
      ],
      "metadata": {
        "id": "UQRWt1NyYg3u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a2fefdb-ae5d-4095-9dee-d26c9f59ee5d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "subset created!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet50CustomInput(nn.Module):\n",
        "    def __init__(self, classes, input_channels):\n",
        "        super(ResNet50CustomInput, self).__init__()\n",
        "\n",
        "        # Load the pre-trained ResNet-50 model without the final classification layer\n",
        "        self.resnet = models.resnet50(pretrained=True)\n",
        "        # Remove the original fully connected layer\n",
        "        self.resnet = nn.Sequential(*list(self.resnet.children())[:-2])\n",
        "\n",
        "        # Modify the first convolution layer to accept smaller input\n",
        "        self.resnet[0] = nn.Conv2d(input_channels, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "\n",
        "        # Adaptive average pooling layer to adapt to different input sizes\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "        # New fully connected layer for your specific number of classes\n",
        "        self.fc = nn.Linear(2048, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.resnet(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "E5lQWtikUq5K"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train function on multiple subsets\n",
        "def train(model, train_loader, optimizer, criterion, epochs):\n",
        "  model.train()\n",
        "  for epoch in range(epochs):\n",
        "    running_loss = 0.0\n",
        "    for i, labels in train_loader:\n",
        "      i, labels = i.to(device), labels.to(device)\n",
        "      optimizer.zero_grad()\n",
        "      outputs = model(i)\n",
        "      loss = criterion(outputs, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      running_loss += loss.item()\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {running_loss/len(train_loader):.4f}\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nu2Cx-8qeENQ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, test_loader):\n",
        "  model.eval()\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  with torch.no_grad():\n",
        "    for i, labels in test_loader:\n",
        "      i, labels = i.to(device), labels.to(device)\n",
        "      outputs = model(i)\n",
        "      _, predicted = torch.max(outputs, 1)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "\n",
        "  return 100 * correct / total"
      ],
      "metadata": {
        "id": "TGu4Ib3thQDx"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cpu\")\n"
      ],
      "metadata": {
        "id": "3mqb5raOrplO"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for subset_name, subset in subsets.items():\n",
        "    print(f\"Subset {subset_name} labels: {set(subset.tensors[1].tolist())}\")"
      ],
      "metadata": {
        "id": "9vsyV6DbryK8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3d46ea1-19d5-44dd-85ac-f9e1bf04af6b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subset 10x10 labels: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\n",
            "Subset 10x100 labels: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\n",
            "Subset 10x200 labels: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\n",
            "Subset 100x10 labels: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99}\n",
            "Subset 100x100 labels: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99}\n",
            "Subset 100x200 labels: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import models\n",
        "\n",
        "results = {}\n",
        "epochs = 50\n",
        "input_channels = 3\n",
        "\n",
        "for subset_name, (train_subset, test_subset) in subsets.items():\n",
        "  print(f\"Training on subset: {subset_name}\")\n",
        "  num_classes = len(set(train_subset.tensors[1].tolist()))\n",
        "  train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
        "  test_loader = DataLoader(test_subset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "  model = ResNet50CustomInput(classes=num_classes, input_channels=input_channels).to(device)\n",
        "  optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  train(model, train_loader, optimizer, criterion, epochs)\n",
        "  accuracy = test(model, test_loader)\n",
        "  print(f\"Accuracy of {subset_name}: {accuracy:.2f}%\")\n",
        "  torch.save(model.state_dict(), f\"/content/drive/MyDrive/models/ResNet50/{subset_name}_ResNet50_model.pth\")\n",
        "  results[subset_name] = accuracy\n",
        "print(\"\\nFinal results:\")\n",
        "for subset_name, accuracy in results.items():\n",
        "    print(f\"{subset_name}: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYYt4atCh4vu",
        "outputId": "cae3a258-b4dd-415a-aff2-fff9b66b96ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on subset: 10x10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 2.3245\n",
            "Epoch 2, Loss: 2.1566\n",
            "Epoch 3, Loss: 1.9241\n",
            "Epoch 4, Loss: 1.8352\n",
            "Epoch 5, Loss: 1.5677\n",
            "Epoch 6, Loss: 1.3659\n",
            "Epoch 7, Loss: 1.3088\n",
            "Epoch 8, Loss: 1.1237\n",
            "Epoch 9, Loss: 1.0896\n",
            "Epoch 10, Loss: 0.9663\n",
            "Epoch 11, Loss: 0.7070\n",
            "Epoch 12, Loss: 0.7008\n",
            "Epoch 13, Loss: 0.5976\n",
            "Epoch 14, Loss: 0.4648\n",
            "Epoch 15, Loss: 0.5973\n",
            "Epoch 16, Loss: 0.3577\n",
            "Epoch 17, Loss: 0.3151\n",
            "Epoch 18, Loss: 0.3775\n",
            "Epoch 19, Loss: 0.2741\n",
            "Epoch 20, Loss: 0.2889\n",
            "Epoch 21, Loss: 0.2738\n",
            "Epoch 22, Loss: 0.2337\n",
            "Epoch 23, Loss: 0.1427\n",
            "Epoch 24, Loss: 0.1027\n",
            "Epoch 25, Loss: 0.1561\n",
            "Epoch 26, Loss: 0.1616\n",
            "Epoch 27, Loss: 0.1178\n",
            "Epoch 28, Loss: 0.1041\n",
            "Epoch 29, Loss: 0.0830\n",
            "Epoch 30, Loss: 0.1645\n",
            "Epoch 31, Loss: 0.1663\n",
            "Epoch 32, Loss: 0.0681\n",
            "Epoch 33, Loss: 0.0780\n",
            "Epoch 34, Loss: 0.0864\n",
            "Epoch 35, Loss: 0.0494\n",
            "Epoch 36, Loss: 0.0736\n",
            "Epoch 37, Loss: 0.0668\n",
            "Epoch 38, Loss: 0.0623\n",
            "Epoch 39, Loss: 0.0645\n",
            "Epoch 40, Loss: 0.0692\n",
            "Epoch 41, Loss: 0.0441\n",
            "Epoch 42, Loss: 0.0337\n",
            "Epoch 43, Loss: 0.0407\n",
            "Epoch 44, Loss: 0.0396\n",
            "Epoch 45, Loss: 0.0262\n",
            "Epoch 46, Loss: 0.0316\n",
            "Epoch 47, Loss: 0.0213\n",
            "Epoch 48, Loss: 0.0442\n",
            "Epoch 49, Loss: 0.0284\n",
            "Epoch 50, Loss: 0.0449\n",
            "Accuracy of 10x10: 50.00%\n",
            "Training on subset: 10x100\n",
            "Epoch 1, Loss: 2.3325\n",
            "Epoch 2, Loss: 1.9373\n",
            "Epoch 3, Loss: 1.5103\n",
            "Epoch 4, Loss: 1.1597\n",
            "Epoch 5, Loss: 0.8534\n",
            "Epoch 6, Loss: 0.6096\n",
            "Epoch 7, Loss: 0.4194\n",
            "Epoch 8, Loss: 0.2678\n",
            "Epoch 9, Loss: 0.1923\n",
            "Epoch 10, Loss: 0.1235\n",
            "Epoch 11, Loss: 0.0910\n",
            "Epoch 12, Loss: 0.0636\n",
            "Epoch 13, Loss: 0.0565\n",
            "Epoch 14, Loss: 0.0516\n",
            "Epoch 15, Loss: 0.0423\n",
            "Epoch 16, Loss: 0.0316\n",
            "Epoch 17, Loss: 0.0286\n",
            "Epoch 18, Loss: 0.0345\n",
            "Epoch 19, Loss: 0.0268\n",
            "Epoch 20, Loss: 0.0214\n",
            "Epoch 21, Loss: 0.0185\n",
            "Epoch 22, Loss: 0.0174\n",
            "Epoch 23, Loss: 0.0136\n",
            "Epoch 24, Loss: 0.0170\n",
            "Epoch 25, Loss: 0.0138\n",
            "Epoch 26, Loss: 0.0163\n",
            "Epoch 27, Loss: 0.0102\n",
            "Epoch 28, Loss: 0.0151\n",
            "Epoch 29, Loss: 0.0170\n",
            "Epoch 30, Loss: 0.0104\n",
            "Epoch 31, Loss: 0.0125\n",
            "Epoch 32, Loss: 0.0119\n",
            "Epoch 33, Loss: 0.0159\n",
            "Epoch 34, Loss: 0.0092\n",
            "Epoch 35, Loss: 0.0127\n",
            "Epoch 36, Loss: 0.0093\n",
            "Epoch 37, Loss: 0.0068\n",
            "Epoch 38, Loss: 0.0100\n",
            "Epoch 39, Loss: 0.0063\n",
            "Epoch 40, Loss: 0.0056\n",
            "Epoch 41, Loss: 0.0062\n",
            "Epoch 42, Loss: 0.0096\n",
            "Epoch 43, Loss: 0.0069\n",
            "Epoch 44, Loss: 0.0059\n",
            "Epoch 45, Loss: 0.0070\n",
            "Epoch 46, Loss: 0.0059\n",
            "Epoch 47, Loss: 0.0061\n",
            "Epoch 48, Loss: 0.0075\n",
            "Epoch 49, Loss: 0.0051\n",
            "Epoch 50, Loss: 0.0042\n",
            "Accuracy of 10x100: 44.50%\n",
            "Training on subset: 10x200\n",
            "Epoch 1, Loss: 2.1485\n",
            "Epoch 2, Loss: 1.4024\n",
            "Epoch 3, Loss: 0.8868\n",
            "Epoch 4, Loss: 0.5488\n",
            "Epoch 5, Loss: 0.3114\n",
            "Epoch 6, Loss: 0.1809\n",
            "Epoch 7, Loss: 0.1074\n",
            "Epoch 8, Loss: 0.0716\n",
            "Epoch 9, Loss: 0.0459\n",
            "Epoch 10, Loss: 0.0352\n",
            "Epoch 11, Loss: 0.0359\n",
            "Epoch 12, Loss: 0.0344\n",
            "Epoch 13, Loss: 0.0240\n",
            "Epoch 14, Loss: 0.0194\n",
            "Epoch 15, Loss: 0.0150\n",
            "Epoch 16, Loss: 0.0158\n",
            "Epoch 17, Loss: 0.0140\n",
            "Epoch 18, Loss: 0.0133\n",
            "Epoch 19, Loss: 0.0109\n",
            "Epoch 20, Loss: 0.0084\n",
            "Epoch 21, Loss: 0.0087\n",
            "Epoch 22, Loss: 0.0076\n",
            "Epoch 23, Loss: 0.0066\n",
            "Epoch 24, Loss: 0.0065\n",
            "Epoch 25, Loss: 0.0072\n",
            "Epoch 26, Loss: 0.0061\n",
            "Epoch 27, Loss: 0.0066\n",
            "Epoch 28, Loss: 0.0090\n",
            "Epoch 29, Loss: 0.0054\n",
            "Epoch 30, Loss: 0.0052\n",
            "Epoch 31, Loss: 0.0065\n",
            "Epoch 32, Loss: 0.0041\n",
            "Epoch 33, Loss: 0.0050\n",
            "Epoch 34, Loss: 0.0057\n",
            "Epoch 35, Loss: 0.0057\n",
            "Epoch 36, Loss: 0.0054\n",
            "Epoch 37, Loss: 0.0059\n",
            "Epoch 38, Loss: 0.0039\n",
            "Epoch 39, Loss: 0.0030\n",
            "Epoch 40, Loss: 0.0033\n",
            "Epoch 41, Loss: 0.0033\n",
            "Epoch 42, Loss: 0.0032\n",
            "Epoch 43, Loss: 0.0031\n",
            "Epoch 44, Loss: 0.0036\n",
            "Epoch 45, Loss: 0.0026\n",
            "Epoch 46, Loss: 0.0024\n",
            "Epoch 47, Loss: 0.0028\n",
            "Epoch 48, Loss: 0.0037\n",
            "Epoch 49, Loss: 0.0021\n",
            "Epoch 50, Loss: 0.0026\n",
            "Accuracy of 10x200: 73.00%\n",
            "Training on subset: 100x10\n",
            "Epoch 1, Loss: 4.7042\n",
            "Epoch 2, Loss: 4.3732\n",
            "Epoch 3, Loss: 4.0327\n",
            "Epoch 4, Loss: 3.7062\n",
            "Epoch 5, Loss: 3.4204\n",
            "Epoch 6, Loss: 3.1012\n",
            "Epoch 7, Loss: 2.8212\n",
            "Epoch 8, Loss: 2.5416\n",
            "Epoch 9, Loss: 2.2688\n",
            "Epoch 10, Loss: 1.9934\n",
            "Epoch 11, Loss: 1.7618\n",
            "Epoch 12, Loss: 1.5500\n",
            "Epoch 13, Loss: 1.3254\n",
            "Epoch 14, Loss: 1.1569\n",
            "Epoch 15, Loss: 0.9677\n",
            "Epoch 16, Loss: 0.7866\n",
            "Epoch 17, Loss: 0.6827\n",
            "Epoch 18, Loss: 0.5568\n",
            "Epoch 19, Loss: 0.4672\n",
            "Epoch 20, Loss: 0.3837\n",
            "Epoch 21, Loss: 0.3269\n",
            "Epoch 22, Loss: 0.2875\n",
            "Epoch 23, Loss: 0.2790\n",
            "Epoch 24, Loss: 0.2138\n",
            "Epoch 25, Loss: 0.2146\n",
            "Epoch 26, Loss: 0.1979\n",
            "Epoch 27, Loss: 0.1608\n",
            "Epoch 28, Loss: 0.1342\n",
            "Epoch 29, Loss: 0.1114\n",
            "Epoch 30, Loss: 0.1052\n",
            "Epoch 31, Loss: 0.1027\n",
            "Epoch 32, Loss: 0.0858\n",
            "Epoch 33, Loss: 0.0808\n",
            "Epoch 34, Loss: 0.0827\n",
            "Epoch 35, Loss: 0.0770\n",
            "Epoch 36, Loss: 0.0752\n",
            "Epoch 37, Loss: 0.0653\n",
            "Epoch 38, Loss: 0.0636\n",
            "Epoch 39, Loss: 0.0599\n",
            "Epoch 40, Loss: 0.0548\n",
            "Epoch 41, Loss: 0.0551\n",
            "Epoch 42, Loss: 0.0530\n",
            "Epoch 43, Loss: 0.0464\n",
            "Epoch 44, Loss: 0.0598\n",
            "Epoch 45, Loss: 0.0514\n",
            "Epoch 46, Loss: 0.0522\n",
            "Epoch 47, Loss: 0.0456\n",
            "Epoch 48, Loss: 0.0468\n",
            "Epoch 49, Loss: 0.0409\n",
            "Epoch 50, Loss: 0.0360\n",
            "Accuracy of 100x10: 11.50%\n",
            "Training on subset: 100x100\n",
            "Epoch 1, Loss: 4.4930\n",
            "Epoch 2, Loss: 3.8143\n",
            "Epoch 3, Loss: 3.1711\n",
            "Epoch 4, Loss: 2.5829\n",
            "Epoch 5, Loss: 2.0317\n",
            "Epoch 6, Loss: 1.5162\n",
            "Epoch 7, Loss: 1.0772\n",
            "Epoch 8, Loss: 0.7097\n",
            "Epoch 9, Loss: 0.4433\n",
            "Epoch 10, Loss: 0.2614\n",
            "Epoch 11, Loss: 0.1551\n",
            "Epoch 12, Loss: 0.1087\n",
            "Epoch 13, Loss: 0.0789\n",
            "Epoch 14, Loss: 0.0647\n",
            "Epoch 15, Loss: 0.0485\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for subset_name, accuracy in results.items():\n",
        "    print(f\"subset {subset_name}: Accuracy = {accuracy:.2f}%\")"
      ],
      "metadata": {
        "id": "b76ZSDxeMx7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((32, 32)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "subsets = {\n",
        "    \"10x10\": 10,\n",
        "    \"10x100\": 10,\n",
        "    \"10x200\": 10,\n",
        "    \"100x10\": 100,\n",
        "    \"100x100\": 100,\n",
        "    \"100x200\": 100,\n",
        "}\n",
        "\n",
        "dataset = datasets.CIFAR100(root='./cifar100', train=True, download=True)\n",
        "image, label = dataset[0]\n",
        "classes = dataset.classes\n",
        "image_np = np.array(image)\n",
        "image_bgr = cv2.cvtColor(image_np, cv2.COLOR_RGB2BGR)\n",
        "input_image = transform(image_np)\n",
        "input_image = input_image.unsqueeze(0).to(device)\n",
        "\n",
        "for subset_name, num_classes in subsets.items():\n",
        "    model = Net(classes=num_classes).to(device)\n",
        "    model.load_state_dict(torch.load(f\"{subset_name}_model.pth\"))\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_image)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "    predicted_class = classes[predicted.item()]\n",
        "    confidence = torch.softmax(outputs, dim=1)[0][predicted.item()].item()\n",
        "    print(f\"Model: {subset_name}\")\n",
        "    print(f\"Predicted Class: {predicted_class}\")\n",
        "    print(f\"Confidence: {confidence:.2f}%\")\n",
        "    plt.imshow(image_np)\n",
        "    plt.title(f'Model: {subset_name}\\nPredicted Class: {predicted_class}\\nConfidence: {confidence:.2f}')\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "C2lf3p8PNHJn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}